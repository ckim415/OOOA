{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Selenium in c:\\users\\shimh\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\shimh\\anaconda3\\lib\\site-packages (from Selenium) (1.25.11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'apt'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'apt-get'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "!pip install Selenium\n",
    "!apt-get update # to update ubuntu to correctly run apt install\n",
    "!apt install chromium-chromedriver\n",
    "!apt-get install -y fonts-nanum*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-182858b766b7>:9: DeprecationWarning: use options instead of chrome_options\n",
      "  webdriver = webdriver.Chrome(r'C:\\chromedriver',chrome_options=options)\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"headless\")\n",
    "# options.add_argument(\"window-size=1500,1200\")\n",
    "options.add_argument(\"no-sandbox\")\n",
    "options.add_argument(\"disable-dev-shm-usage\")\n",
    "# options.add_argument(\"disable-gpu\")\n",
    "# options.add_argument(\"log-level=3\")\n",
    "webdriver = webdriver.Chrome(r'C:\\chromedriver',chrome_options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#스크롤 이동하며 링크 추출하는 함수이다.\n",
    "#url과, 내보낼 파일 이름이 변수로 들어간다.\n",
    "\n",
    "def link_extractor(url,file_name):\n",
    "    webdriver.get(url)\n",
    "    time.sleep(5)\n",
    "    last_height = webdriver.execute_script(\"return document.body.scrollHeight\")\n",
    "    body = webdriver.find_element_by_tag_name(\"body\")\n",
    "    \n",
    "    #더보기 버튼이 있으면 실행!! 데이터가 너무 적은 카테고리 때문에 예외처리 하였다.\n",
    "    try:\n",
    "        btn = webdriver.find_element_by_css_selector('#main-app > div.MainWrapper_content__GZkTa > div > div.RewardProjectListApp_container__1ZYeD.RewardMainProjectList_listApp__2noRS > div.ProjectCardList_container__3Y14k > div:nth-child(2) > div > button')\n",
    "        \n",
    "        #스크롤 높이 차를 이용한 로직\n",
    "        while True:\n",
    "            try:\n",
    "                body.send_keys(Keys.PAGE_UP)\n",
    "                time.sleep(20)\n",
    "                webdriver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-50);\")\n",
    "                time.sleep(20)\n",
    "\n",
    "            except:\n",
    "                print('버튼 안됨/타임오류')\n",
    "                break\n",
    "                \n",
    "            #현재 스크롤의 높이를 기록한다.\n",
    "            new_height = webdriver.execute_script(\"return document.body.scrollHeight\")\n",
    "            \n",
    "            #반복문 탈출 조건! 이전과 현재의 스크롤 높이를 비교하여 같으면 링크 추출로 넘어간다\n",
    "            if (new_height == last_height):\n",
    "                # 그런데 데이터가 많은 카테고리인 경우 렉때문에 버튼이 존재하는데도 스크롤이 안내려가는 경우가 있어, 이를 방지한 추가 조건이다.\n",
    "                if btn:\n",
    "                    continue\n",
    "                print(\"데이터 내보내기 과정으로 이동하겠습니다/\")\n",
    "                break\n",
    "            last_height = new_height\n",
    "            \n",
    "\n",
    "        #스크롤 끝까지 다 내리고 csv로 내보내기\n",
    "        try:\n",
    "            atag = webdriver.find_elements_by_css_selector(\"#main-app > div.MainWrapper_content__GZkTa > div > div.RewardProjectListApp_container__1ZYeD.RewardMainProjectList_listApp__2noRS > div.ProjectCardList_container__3Y14k > div.ProjectCardList_list__1YBa2 > div a\")\n",
    "            list_href = [a.get_attribute('href') for a in atag]\n",
    "            df = pd.DataFrame(list_href)\n",
    "            df.drop_duplicates(ignore_index = True,inplace=True)\n",
    "            df.to_csv(f'{file_name}.csv')    \n",
    "        except:\n",
    "            print('태그 업서요')\n",
    "            \n",
    "            \n",
    "    #더보기 버튼이 없으면! 스크롤 안내리고 바로 데이터 추출       \n",
    "    except:\n",
    "        try:\n",
    "            atag = webdriver.find_elements_by_css_selector(\"#main-app > div.MainWrapper_content__GZkTa > div > div.RewardProjectListApp_container__1ZYeD.RewardMainProjectList_listApp__2noRS > div.ProjectCardList_container__3Y14k > div.ProjectCardList_list__1YBa2 > div a\")\n",
    "            list_href = [a.get_attribute('href') for a in atag]\n",
    "            df = pd.DataFrame(list_href)\n",
    "            df.drop_duplicates(ignore_index = True,inplace=True)\n",
    "            df.to_csv(f'{file_name}.csv')    \n",
    "        except:\n",
    "            print('태그 업서요')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_extractor(file_name):\n",
    "\n",
    "    titles, categories, company_names, percentages, funding_amounts, days_left = [],[],[],[],[],[]\n",
    "    n_supporters, likes = [],[]\n",
    "    durations, target_amounts = [],[]\n",
    "    descriptions = []\n",
    "    visited = []\n",
    "    error = []\n",
    "\n",
    "    link_list= list(pd.read_csv(file_name+\".csv\", sep=',').iloc[:,1])\n",
    "    \n",
    "    while link_list:\n",
    "        try:\n",
    "            #리스트에서 맨 앞에 요소 빼오기\n",
    "            url = link_list.pop(0)\n",
    "            #방문한 리스트 저장\n",
    "            visited.append(url)\n",
    "\n",
    "            soup = bs(requests.get(url).text, 'html.parser')\n",
    "\n",
    "            #titles, categories\n",
    "            category = soup.find('p',class_ = 'title-info').text\n",
    "            categories.append(category)\n",
    "            title = soup.find('h2',class_ = 'title').text\n",
    "            titles.append(title)\n",
    "\n",
    "            #days_left, percentages, funding_amounts, n_supporters\n",
    "            info = soup.select_one('#container > div.reward-body-wrap > div > div.wd-ui-info-wrap > div.wd-ui-sub-campaign-info-container > div > div > section > div.wd-ui-campaign-content > div > div.project-state-info > div.state-box').text.split('\\n')\n",
    "            day = info[1]\n",
    "            days_left.append(day)\n",
    "            percentage = info[3].split(' ')[0]\n",
    "            percentages.append(percentage)\n",
    "            funding = info[4].split(' ')[0]\n",
    "            funding_amounts.append(funding)\n",
    "            supporter = info[5].split(' ')[0].split('명')[0]\n",
    "            n_supporters.append(supporter)\n",
    "\n",
    "            #company_names \n",
    "            company_name = soup.find(\"div\", {\"id\":\"reward-report-content2\"})['data-maker-name']\n",
    "            company_names.append(company_name)\n",
    "\n",
    "            #likes\n",
    "            like = soup.select_one('#cntLike').text\n",
    "            likes.append(like)\n",
    "\n",
    "            #durations,target_amounts\n",
    "            info = soup.find('p',style='color:#00b2b2;font-size:13px;line-height:20px;margin-bottom:8px;').text.split('\\n')\n",
    "            duration = info[2].split(' ')[-1]\n",
    "            durations.append(duration)\n",
    "            target_amount = info[1].split(' ')[-1]\n",
    "            target_amounts.append(target_amount)\n",
    "\n",
    "            #descriptions\n",
    "            description = soup.select_one(\"#container > div.reward-body-wrap > div > div.wd-ui-info-wrap > div.wd-ui-sub-campaign-info-container > div > div > section > div.campaign-summary\")\n",
    "            descriptions.append(description.text.split('\\n')[1].strip())\n",
    "\n",
    "\n",
    "        except:\n",
    "            error.append(url)\n",
    "            continue\n",
    "    for i in [titles, categories,days_left, percentages, funding_amounts, n_supporters, company_names,likes,durations,target_amounts,description]:\n",
    "        print(len(i))\n",
    "        print(i)\n",
    "        \n",
    "    data = {\n",
    "    'categories':categories,\n",
    "    'title':titles,\n",
    "    \"company_names\":company_names,\n",
    "    'percentages':percentages, \n",
    "    'funding_amounts' :funding_amounts, \n",
    "    'days_left' :days_left\n",
    "                } \n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['n_supporters'] = n_supporters\n",
    "    df['likes'] = likes\n",
    "    df['descriptions'] = descriptions\n",
    "    df['durations'] = durations\n",
    "    df['target_amounts'] = target_amounts\n",
    "    \n",
    "    \n",
    "    df.to_csv(f'{file_name}에서 추출한 데이터.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "['[영화모임] 한 편의 영화 같은 3개월, 담화관 시즌모임으로 초대합니다!', '[5.0점앵콜] 퇴직/이직 전 진짜 내가 하고 싶은 일을 찾는 곳ㅣ사하라', '퇴직/이직 하기 전, 진짜 내가 하고 싶은 일을 찾는 곳 <사하라>', \"이런 소셜살롱 보셨나요?   영화테마살롱 -  컴'온무비\", '완소균이부터 코스토리까지 과거의 경험, 미래의 도전을 공유합니다!', '맥주로 만나는 새로운 경험! 비어스픽 프로그램 멤버십', '[온라인앵콜]퇴사/이직/부업/투잡 전 진짜 내가 하고 싶은 일을 찾는 법', '습관의 탄생 by 생각식당 개업 1주년 기념', \"라이프쉐어 다이빙 클럽 - ME '나는 나를 얼마나 알고 있을까요?'\", '언젠가 홀로 설 당신에게 보내는 \"선택\" 이야기 [가수 류세라]', '직장인의 딴짓을 돕는 멤버십커뮤니티, 딴짓클럽 ', '[시크릿사파리] 블랙뮤직과 춤을 사랑하는 2030의 신나는 연말파티!', '나는 무엇을 좋아할까요? by 인문,예술 모임공간 CABINET', '\"육아는 누구나 처음이라\" 양육자를 위한 커뮤니티 <123swing>', \"라이프쉐어 다이빙 클럽 - WORK '내게 맞는 일은 무엇일까요?'\", \"라이프쉐어 다이빙클럽 - LOVE  '내 사랑은 왜 어려울까요?'\", '당신의 \"퇴근후2시간\",인사라운지에서 열리는 특별한 모임에 참여하세요!', \"라이프쉐어 다이빙 클럽 - TRAVEL '머무는 삶 vs 떠도는 삶'\", '성찰이 필요할 때, 마음을 서예로 만나는 시간 [서예가 인중 이정화]', '두어블, 40,50대를 위한 취미, 배움, 모임 플랫폼 개발 프로젝트', '(12명 정예  여성멤버 모집) 미국 부동산투자 여성사업가  변신프로젝트', \"똑똑! 혹시 '귀촌' 생각 여기있나요?\", '스트레스 [끊다] 익명 프리미엄 모임 / 3분기 시즌권 / 속시원하게풀자', '떼구르르 보드게임여행', '100여명의 창작자들이 거쳐간 아티스트들을 위한 공간, 아트라우드', '스트라디움 GSML 서포터즈 모집']\n",
      "26\n",
      "['모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임', '모임']\n",
      "26\n",
      "['펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '펀딩성공', '81% 달성', '66% 달성', '55% 달성', '12% 달성', '9% 달성', '0% 달성', '0% 달성']\n",
      "26\n",
      "['1136%', '805%', '582%', '579%', '477%', '468%', '315%', '247%', '245%', '189%', '175%', '158%', '149%', '143%', '121%', '120%', '110%', '101%', '100%', '15명의', '3명의', '32명의', '5명의', '2명의', '10명의', '3명의']\n",
      "26\n",
      "['5,681,000원', '8,050,000원', '5,820,000원', '2,895,000원', '2,385,000원', '4,684,000원', '3,150,000원', '2,475,001원', '2,940,000원', '1,060,000원', '1,750,000원', '790,000원', '3,264,000원', '2,865,000원', '1,460,000원', '2,050,000원', '553,000원', '1,180,000원', '560,000원', '', '', '', '', '', '', '']\n",
      "19\n",
      "['41', '24', '13', '36', '81', '99', '11', '19', '16', '23', '32', '25', '29', '29', '6', '7', '18', '7', '13']\n",
      "19\n",
      "['영화 그 후의 이야기, 담화관', '사하라', '사하라', \"(주)컴'온살롱\", 'ABT', '비어스픽', '주식회사 사하라라이프', '생각식당 (김우정)', '라이프쉐어', '라이프온어스', '딴짓클럽 ', '카리브사파리 (최정아)', '카비네', '주식회사 콜렉티브', '라이프쉐어', '라이프쉐어', '퇴근후2시간', '라이프쉐어', '라이프온어스']\n",
      "19\n",
      "['67', '84', '65', '46', '22', '46', '24', '14', '27', '21', '57', '36', '31', '25', '14', '14', '33', '14', '20']\n",
      "19\n",
      "['2020.01.29-2020.02.19', '2019.12.18-2019.12.29', '2019.08.07-2019.08.25', '2019.12.18-2020.01.15', '2019.10.18-2019.10.27', '2019.10.25-2019.11.03', '2020.09.25-2020.10.05', '2019.07.03-2019.07.21', '2019.04.08-2019.04.22', '2019.04.12-2019.04.21', '2019.10.01-2019.10.17', '2019.12.03-2019.12.15', '2019.10.03-2019.10.31', '2019.08.21-2019.09.01', '2019.04.08-2019.04.22', '2019.04.08-2019.04.22', '2019.08.19-2019.09.08', '2019.04.08-2019.04.22', '2019.04.12-2019.04.21']\n",
      "19\n",
      "['500,000원', '1,000,000원', '1,000,000원', '500,000원', '500,000원', '1,000,000원', '1,000,000원', '1,000,000원', '1,200,000원', '560,000원', '1,000,000원', '500,000원', '2,180,000원', '2,000,000원', '1,200,000원', '1,700,000원', '500,000원', '1,160,000원', '560,000원']\n",
      "1\n",
      "<div class=\"campaign-summary\" style=\"background-color: #ffffff;\">\n",
      "    진짜 하고 싶다고 생각했는데,  왜 나는 열심히 하지 못할까?\r\n",
      "삶이 내 마음 같지 않을 때, 서예를 통해 마음과 만나는 시간. \r\n",
      "서예가 이정화의 \"OnUS \"로 초대합니다.\n",
      "    \n",
      "  </div>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3ccd6394b70b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# category_list =\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0minfo_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"모임\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-5cfd2bd98ccd>\u001b[0m in \u001b[0;36minfo_extractor\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m     72\u001b[0m                 } \n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_supporters'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_supporters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'likes'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlikes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "link_extractor(url,\"모임\"):  # 파일 이름은 자유롭게 설정해주시면 됩니다.\n",
    "\n",
    "info_extractor(\"모임\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-506aea03beb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"categories\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
